{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from keras.layers import Conv2D, Dense, Activation\n",
    "from functools import reduce\n",
    "from keras.models import Sequential\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting directory list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = Image.open('./fish/220px-Alligator_Gar_10.JPG.jpg', \"r\")\n",
    "d = 'fish/'\n",
    "# d = 'created-data/'\n",
    "dirs = [d + x.strip(' ') + '/' for x in os.listdir(d)]\n",
    "temp = []\n",
    "\n",
    "# Removing DS store\n",
    "for d in dirs:\n",
    "    if d == 'fish/.DS_Store':\n",
    "        continue\n",
    "    temp.append(d)\n",
    "dirs = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading each image in a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading images for: fish/bluegill/\n",
      "Start loading images for: fish/smallmouth_bass/\n",
      "Start loading images for: fish/blue_catfish/\n",
      "Start loading images for: fish/walleye/\n",
      "Start loading images for: fish/black_crappie/\n",
      "Start loading images for: fish/channel_catfish/\n",
      "Start loading images for: fish/Green Sunfish/\n",
      "Start loading images for: fish/alligator_gar/\n",
      "Start loading images for: fish/rainbow_trout/\n",
      "Start loading images for: fish/common_carp/\n",
      "Start loading images for: fish/northern_pike/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 9. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading images for: fish/Largemouth Bass/\n"
     ]
    }
   ],
   "source": [
    "def get_image_dict():\n",
    "    fish_data = {}\n",
    "\n",
    "    for directory in dirs:\n",
    "        try:\n",
    "            dir_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "\n",
    "        images = []\n",
    "        print(\"Start loading images for: \" + directory)\n",
    "        for image_file in dir_files:\n",
    "            try:\n",
    "                f = Image.open(directory + image_file)\n",
    "                img_as_arr = np.array(f.resize((256,256)))\n",
    "                images.append(img_as_arr)\n",
    "                f.close()\n",
    "            except OSError:\n",
    "                continue\n",
    "        fish_data[directory.split(\"/\")[1]] = images\n",
    "    return fish_data\n",
    "\n",
    "fish_data = get_image_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vec(fish_data):\n",
    "    \"\"\"\n",
    "    :returns: x_train, y_train\n",
    "    \"\"\"\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for key in fish_data:\n",
    "        data = fish_data[key]\n",
    "        \n",
    "        samples = []\n",
    "        for image in data:\n",
    "            if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "                continue\n",
    "            samples.append(image)\n",
    "\n",
    "        shape = (len(samples),) + data[0].shape\n",
    "\n",
    "        x_placeholder = np.zeros(shape)\n",
    "        \n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            x_placeholder[i] = sample\n",
    "        \n",
    "        x_train.append(x_placeholder)\n",
    "        y_train.append(key)\n",
    "        \n",
    "    return x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec, labels = dict_to_vec(fish_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped_list = list(zip(x_train_vec, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , [labels[0]]\n",
    "# list(zip(x_train_vec, labels))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(zippped_list, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open('training_data.pkl', 'r') as f:\n",
    "#     pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing walleye\n",
    "# if x_train_vec[3].shape[0] < 1000:\n",
    "#     x_train_vec.pop(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 256, 256, 3),\n",
       " (49, 256, 256, 3),\n",
       " (68, 256, 256, 3),\n",
       " (32, 256, 256, 3),\n",
       " (52, 256, 256, 3),\n",
       " (52, 256, 256, 3),\n",
       " (56, 256, 256, 3),\n",
       " (60, 256, 256, 3),\n",
       " (41, 256, 256, 3),\n",
       " (42, 256, 256, 3),\n",
       " (67, 256, 256, 3),\n",
       " (42, 256, 256, 3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in x_train_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_index = []\n",
    "for vec in x_train_vec:\n",
    "    list_num_index.append(vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(x_train_vec)):\n",
    "#     x_train_vec[i] = x_train_vec[i][0:1000,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list_num_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "for i, num_examples in enumerate(list_num_index):\n",
    "    y.append([i] * num_examples)\n",
    "# flatten\n",
    "\n",
    "y_train = np.array([item for sublist in y for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 256, 256, 3)\n",
      "(92, 256, 256, 3)\n",
      "(519,)\n",
      "(92,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('keras_model/x_train.npy', x_train)\n",
    "# np.save('keras_model/y_train.npy', y)\n",
    "\n",
    "with open('keras_model/train100.pkl', 'wb') as handle:\n",
    "    pickle.dump((X_train, Y_train), handle)\n",
    "    \n",
    "with open('keras_model/test100.pkl', 'wb') as handle:\n",
    "    pickle.dump((X_test, Y_test), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('keras_model/train2.pkl', 'wb') as handle:\n",
    "#     pickle.dump((x_train[5000:], y[5000:]), handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('keras_model/test.pkl', 'wb') as handle:\n",
    "#     pickle.dump((X_test, y_test), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(611, 256, 256, 3)\n",
      "(611,)\n"
     ]
    }
   ],
   "source": [
    "# pickle.dump(x_train, 'x_train')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'channels': 3,\n",
    "    'window': 5,\n",
    "    'stride': 3,\n",
    "    'padding': 'same',\n",
    "    'activation': 'relu'\n",
    "}\n",
    "\n",
    "channels = parameters['channels']\n",
    "window = parameters['window']\n",
    "stride = parameters['stride']\n",
    "padding = parameters['padding']\n",
    "activation = parameters['activation']\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# magic numbers: 256 * 256 * 3 is the total pixels per image\n",
    "model.add(Dense((256 * 256 * 3), input_shape=x_train.shape))\n",
    "\n",
    "model.add(Conv2D(filters=channels, kernel_size=window, strides=stride, padding=padding, activation=activation,\n",
    "    data_format='channels_last'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
