{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "from keras.layers import Conv2D, Dense, Activation\n",
    "from functools import reduce\n",
    "from keras.models import Sequential\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting directory list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = Image.open('./fish/220px-Alligator_Gar_10.JPG.jpg', \"r\")\n",
    "d = 'fish/'\n",
    "# d = 'created-data/'\n",
    "dirs = [d + x.strip(' ') + '/' for x in os.listdir(d)]\n",
    "temp = []\n",
    "\n",
    "# Removing DS store\n",
    "for d in dirs:\n",
    "    if d == 'fish/.DS_Store':\n",
    "        continue\n",
    "    temp.append(d)\n",
    "dirs = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading each image in a directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading images for: fish/bluegill/\n",
      "Start loading images for: fish/smallmouth_bass/\n",
      "Start loading images for: fish/blue_catfish/\n",
      "Start loading images for: fish/walleye/\n",
      "Start loading images for: fish/black_crappie/\n",
      "Start loading images for: fish/channel_catfish/\n",
      "Start loading images for: fish/Green Sunfish/\n",
      "Start loading images for: fish/alligator_gar/\n",
      "Start loading images for: fish/rainbow_trout/\n",
      "Start loading images for: fish/common_carp/\n",
      "Start loading images for: fish/northern_pike/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 9. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading images for: fish/Largemouth Bass/\n"
     ]
    }
   ],
   "source": [
    "def get_image_dict():\n",
    "    fish_data = {}\n",
    "\n",
    "    for directory in dirs:\n",
    "        try:\n",
    "            dir_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "        except NotADirectoryError:\n",
    "            continue\n",
    "\n",
    "        images = []\n",
    "        print(\"Start loading images for: \" + directory)\n",
    "        for image_file in dir_files:\n",
    "            try:\n",
    "                f = Image.open(directory + image_file)\n",
    "                img_as_arr = np.array(f.resize((256,256)))\n",
    "                images.append(img_as_arr)\n",
    "                f.close()\n",
    "            except OSError:\n",
    "                continue\n",
    "        fish_data[directory.split(\"/\")[1]] = images\n",
    "    return fish_data\n",
    "\n",
    "fish_data = get_image_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vec(fish_data):\n",
    "    \"\"\"\n",
    "    :returns: x_train, y_train\n",
    "    \"\"\"\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    for key in fish_data:\n",
    "        data = fish_data[key]\n",
    "        \n",
    "        samples = []\n",
    "        for image in data:\n",
    "            if len(image.shape) != 3 or image.shape[2] != 3:\n",
    "                continue\n",
    "            samples.append(image)\n",
    "\n",
    "        shape = (len(samples),) + data[0].shape\n",
    "\n",
    "        x_placeholder = np.zeros(shape)\n",
    "        \n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            x_placeholder[i] = sample\n",
    "        \n",
    "        x_train.append(x_placeholder)\n",
    "        y_train.append(key)\n",
    "        \n",
    "    return x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vec, labels = dict_to_vec(fish_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipped_list = list(zip(x_train_vec, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# , [labels[0]]\n",
    "# list(zip(x_train_vec, labels))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('training_data.pkl', 'wb') as f:\n",
    "#     pickle.dump(zippped_list, f)\n",
    "#     f.close()\n",
    "\n",
    "# with open('training_data.pkl', 'r') as f:\n",
    "#     pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing walleye\n",
    "# if x_train_vec[3].shape[0] < 1000:\n",
    "#     x_train_vec.pop(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50, 256, 256, 3),\n",
       " (49, 256, 256, 3),\n",
       " (68, 256, 256, 3),\n",
       " (32, 256, 256, 3),\n",
       " (52, 256, 256, 3),\n",
       " (52, 256, 256, 3),\n",
       " (56, 256, 256, 3),\n",
       " (60, 256, 256, 3),\n",
       " (41, 256, 256, 3),\n",
       " (42, 256, 256, 3),\n",
       " (67, 256, 256, 3),\n",
       " (42, 256, 256, 3)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in x_train_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_num_index = []\n",
    "for vec in x_train_vec:\n",
    "    list_num_index.append(vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(x_train_vec)):\n",
    "#     x_train_vec[i] = x_train_vec[i][0:1000,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(list_num_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "\n",
    "for i, num_examples in enumerate(list_num_index):\n",
    "    y.append([i] * num_examples)\n",
    "# flatten\n",
    "\n",
    "y_train = np.array([item for sublist in y for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate(x_train_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(519, 256, 256, 3)\n",
      "(92, 256, 256, 3)\n",
      "(519,)\n",
      "(92,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('keras_model/x_train.npy', x_train)\n",
    "np.save('keras_model/y_train.npy', y)\n",
    "\n",
    "# with open('keras_model/train100.pkl', 'wb') as handle:\n",
    "#     pickle.dump((X_train, Y_train), handle)\n",
    "    \n",
    "# with open('keras_model/test100.pkl', 'wb') as handle:\n",
    "#     pickle.dump((X_test, Y_test), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('keras_model/train2.pkl', 'wb') as handle:\n",
    "#     pickle.dump((x_train[5000:], y[5000:]), handle, protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('keras_model/test.pkl', 'wb') as handle:\n",
    "#     pickle.dump((X_test, y_test), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(611, 256, 256, 3)\n",
      "(611,)\n"
     ]
    }
   ],
   "source": [
    "# pickle.dump(x_train, 'x_train')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters = {\n",
    "#     'channels': 3,\n",
    "#     'window': 5,\n",
    "#     'stride': 3,\n",
    "#     'padding': 'same',\n",
    "#     'activation': 'relu'\n",
    "# }\n",
    "\n",
    "# channels = parameters['channels']\n",
    "# window = parameters['window']\n",
    "# stride = parameters['stride']\n",
    "# padding = parameters['padding']\n",
    "# activation = parameters['activation']\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# # magic numbers: 256 * 256 * 3 is the total pixels per image\n",
    "# model.add(Dense((256 * 256 * 3), input_shape=x_train.shape))\n",
    "\n",
    "# model.add(Conv2D(filters=channels, kernel_size=window, strides=stride, padding=padding, activation=activation,\n",
    "#     data_format='channels_last'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = keras.engine.topology.Input(shape=x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.applications.inception_v3.InceptionV3(include_Top=False)\n",
    "model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet', input_tensor=x_input, input_shape=x_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer concatenate_9 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.core.Dense'>. Full input: [<tf.Tensor 'mixed10_1/concat:0' shape=(?, 6, 6, 2048) dtype=float32>, <keras.layers.core.Dense object at 0x1410a3a20>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    464\u001b[0m                           tf.SparseTensor)):\n\u001b[0;32m--> 465\u001b[0;31m         raise ValueError('Unexpectedly found an instance of type `' + str(type(x)) + '`. '\n\u001b[0m\u001b[1;32m    466\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.layers.core.Dense'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-2a8908fd5464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(inputs, axis, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mconcatenation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minputs\u001b[0m \u001b[0malongside\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m     \"\"\"\n\u001b[0;32m--> 641\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer concatenate_9 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.core.Dense'>. Full input: [<tf.Tensor 'mixed10_1/concat:0' shape=(?, 6, 6, 2048) dtype=float32>, <keras.layers.core.Dense object at 0x1410a3a20>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "dense = Dense(len(labels))\n",
    "output = keras.layers.concatenate([model.outputs[0], dense], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
